import os
from dotenv import load_dotenv
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.models.base_model import DeepEvalBaseLLM
from deepeval.metrics.g_eval import Rubric

# [Import] 1. ì‚¬ìš©ìê°€ ì •ì˜í•œ LLM ì„œë¹„ìŠ¤
from src.llm_service import LLM_gemini

# [Import] 2. ë²•ë ¹ ë° íŒë¡€ DB ë§¤ë‹ˆì €
# (ê²½ë¡œ ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ì •í™•í•œ ìœ„ì¹˜ì—ì„œ import)
from src.law.legal_context import LawContextManager
from src.law.precedent_context import PrecedentContextManager

load_dotenv()

# --- 1. DeepEvalìš© Gemini ì–´ëŒ‘í„° (Adapter) ---
class GeminiDeepEvalAdapter(DeepEvalBaseLLM):
    """
    'LLM_gemini' í´ë˜ìŠ¤ë¥¼ 'DeepEval' ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ
    ë³€í™˜í•´ì£¼ëŠ” ì—°ê²° ê³ ë¦¬(Adapter) í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    """
    def __init__(self, llm_service: LLM_gemini):
        # ì´ë¯¸ ìƒì„±ëœ LLM_gemini ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°›ì•„ì„œ ì €ì¥í•©ë‹ˆë‹¤.
        self.llm_service = llm_service
        self.model_name = llm_service.model_name

    def load_model(self):
        # DeepEvalì´ ëª¨ë¸ ê°ì²´ë¥¼ ìš”ì²­í•  ë•Œ clientë¥¼ ë°˜í™˜
        return self.llm_service.client

    def generate(self, prompt: str) -> str:
        """
        DeepEvalì´ í‰ê°€ë¥¼ ìœ„í•´ í…ìŠ¤íŠ¸ ìƒì„±ì„ ìš”ì²­í•  ë•Œ í˜¸ì¶œë˜ëŠ” í•¨ìˆ˜
        """
        # 1. llm_serviceì˜ generate í•¨ìˆ˜ í˜¸ì¶œ (Response ê°ì²´ ë°˜í™˜ë¨)
        response = self.llm_service.generate(prompt)
        
        # 2. Response ê°ì²´ì—ì„œ í…ìŠ¤íŠ¸(.text)ë§Œ ì¶”ì¶œí•˜ì—¬ ë¬¸ìì—´ë¡œ ë°˜í™˜
        return response.text

    async def a_generate(self, prompt: str) -> str:
        # ë¹„ë™ê¸° í˜¸ì¶œ ì‹œì—ë„ ë™ê¸° í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (Gemini Python SDK íŠ¹ì„±)
        return self.generate(prompt)

    def get_model_name(self):
        return self.model_name

# --- 2. ë…ì†Œì¡°í•­ íŒë³„ê¸° í´ë˜ìŠ¤ ---
class ToxicClauseDetector:
    def __init__(self, gemini_api: str = None):
        print("ğŸ›¡ï¸ ToxicClauseDetector ë° DB ì´ˆê¸°í™” ì¤‘...")
        
        # 1. LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        api_key = os.getenv("GEMINI_API_KEY")
        # í‰ê°€ì˜ ì •í™•ë„ë¥¼ ìœ„í•´ ê°€ê¸‰ì  'pro' ëª¨ë¸ ì‚¬ìš© ê¶Œì¥
        # llm_service.pyì˜ LLM_gemini í´ë˜ìŠ¤ ì‚¬ìš©
        self.llm_service = LLM_gemini(gemini_api_key=api_key, model="gemini-2.5-flash")
        
        # 2. ì–´ëŒ‘í„° ì—°ê²° (DeepEval í‰ê°€ìš©)
        self.evaluator_llm = GeminiDeepEvalAdapter(self.llm_service)
        
        # 3. DB ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.law_manager = LawContextManager()
        self.precedent_manager = PrecedentContextManager()
        
        # DB ë¡œë“œ (ìµœì´ˆ ì‹¤í–‰ ì‹œì—ë§Œ ë¡œë”©/êµ¬ì¶•)
        self.law_manager.initialize_database()
        self.precedent_manager.initialize_database()

        # 4. G-Eval í‰ê°€ ê¸°ì¤€ (Rubric) ì •ì˜
        self.toxic_criteria = """
        ë‹¹ì‹ ì€ í•œêµ­ì˜ ê·¼ë¡œê¸°ì¤€ë²•ì„ ìˆ˜í˜¸í•˜ëŠ” ì—„ê²©í•œ 'ê·¼ë¡œê³„ì•½ì„œ ê°ì‚¬ê´€'ì…ë‹ˆë‹¤.
        ì…ë ¥ëœ 'ê·¼ë¡œê³„ì•½ ì¡°í•­'ì´ ì œê³µëœ 'ê´€ë ¨ ë²•ë ¹/íŒë¡€(Context)'ì— ë¹„ì¶”ì–´ ë³¼ ë•Œ 
        ê·¼ë¡œìì—ê²Œ ë¶ˆë¦¬í•˜ê±°ë‚˜, ë¶ˆë²•ì ì´ê±°ë‚˜, ë…ì†Œì¡°í•­(Toxic Clause)ì— í•´ë‹¹í•˜ëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.

        [ë…ì†Œì¡°í•­ íŒë‹¨ ê¸°ì¤€]
        1. ê°•í–‰ê·œì • ìœ„ë°˜: ìµœì €ì„ê¸ˆ ë¯¸ë‹¬, í‡´ì§ê¸ˆ í¬ê¸° ê°ì„œ, ìœ„ì•½ê¸ˆ ì˜ˆì • ë“± ë²•ìœ¼ë¡œ ê¸ˆì§€ëœ ë‚´ìš©ì¸ê°€?
        2. í¬ê´„ì„ê¸ˆ ì˜¤ë‚¨ìš©: ê·¼ë¡œì‹œê°„ ì‚°ì •ì´ ê°€ëŠ¥í•œë°ë„ í¬ê´„ì„ê¸ˆì œë¥¼ ì ìš©í•˜ì—¬ ìˆ˜ë‹¹ì„ ë¯¸ì§€ê¸‰í•˜ë ¤ í•˜ëŠ”ê°€?
        3. ë¶ˆê³µì •ì„±: 'ê°‘'ì—ê²Œ ì¼ë°©ì ìœ¼ë¡œ ìœ ë¦¬í•˜ê±°ë‚˜, ëª¨í˜¸í•œ í‘œí˜„ìœ¼ë¡œ 'ì„'ì˜ ê¶Œë¦¬ë¥¼ ì œí•œí•˜ëŠ”ê°€?
        4. ì ˆì°¨ ë¬´ì‹œ: í•´ê³ , ì§•ê³„ ë“±ì˜ ì ˆì°¨ë¥¼ ë²•ì  ê¸°ì¤€ë³´ë‹¤ ê°„ì†Œí™”í•˜ê±°ë‚˜ ìƒëµí•˜ëŠ”ê°€?
        """

        # ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ 'ì•ˆì „(Safe)'í•œ ê²ƒìœ¼ë¡œ ì„¤ì •
        self.rubric = [
            Rubric(score_range=(0,2), expected_outcome="ë²•ì  íš¨ë ¥ì´ ì—†ê±°ë‚˜ ê·¼ë¡œìì—ê²Œ ì‹¬ê°í•˜ê²Œ ë¶ˆë¦¬í•œ ë…ì†Œì¡°í•­."),
            Rubric(score_range=(3,5), expected_outcome="ë‹¤íˆ¼ì˜ ì—¬ì§€ê°€ ìˆê±°ë‚˜ ê·¼ë¡œìì—ê²Œ ë¶ˆë¦¬í•˜ê²Œ í•´ì„ë  ìˆ˜ ìˆëŠ” ì¡°í•­."),
            Rubric(score_range=(6,7), expected_outcome="ëŒ€ì²´ë¡œ ê³µì •í•˜ì§€ë§Œ ì¼ë¶€ í‘œí˜„ì´ ëª¨í˜¸í•œ ì¡°í•­."),
            Rubric(score_range=(8,10), expected_outcome="ê´€ë ¨ ë²•ë ¹ê³¼ íŒë¡€ë¥¼ ì™„ë²½íˆ ì¤€ìˆ˜í•˜ëŠ” ì•ˆì „í•œ ì¡°í•­."),
        ]

        self.evaluation_steps = [
            "ì…ë ¥ëœ 'ê³„ì•½ ì¡°í•­'ì˜ í•µì‹¬ ë‚´ìš©ì„ íŒŒì•…í•œë‹¤.",
            "ì œê³µëœ 'Context(ë²•ë ¹/íŒë¡€)'ì™€ ì¡°í•­ì„ ëŒ€ì¡°í•˜ì—¬ ë²•ì  ìµœì € ê¸°ì¤€(Minimum Standard) ì¤€ìˆ˜ ì—¬ë¶€ë¥¼ í™•ì¸í•œë‹¤.",
            "ì¡°í•­ì— 'ìœ„ì•½ê¸ˆ', 'í¬ê¸°', 'ë¯¼í˜•ì‚¬ìƒ ì´ì˜ ì œê¸° ê¸ˆì§€' ë“± ë¶ˆë²•ì  í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸í•œë‹¤.",
            "ë²• ìœ„ë°˜ ì‚¬í•­ì´ ìˆìœ¼ë©´ ë‚®ì€ ì ìˆ˜(ìœ„í—˜)ë¥¼, ì¤€ìˆ˜í–ˆë‹¤ë©´ ë†’ì€ ì ìˆ˜(ì•ˆì „)ë¥¼ ë¶€ì—¬í•œë‹¤."
        ]

    def _retrieve_context(self, clause_text):
        """
        ë²•ë ¹ê³¼ íŒë¡€ë¥¼ DBì—ì„œ ê²€ìƒ‰í•˜ì—¬ í”„ë¡¬í”„íŠ¸ìš© ë¬¸ìì—´ë¡œ ë°˜í™˜
        """
        # 1. ë²•ë ¹ ê²€ìƒ‰
        laws = self.law_manager.search_relevant_laws(clause_text, k=2)
        law_text = "\n".join(laws) if laws else "ê´€ë ¨ ë²•ë ¹ ì—†ìŒ"

        # 2. íŒë¡€ ê²€ìƒ‰
        precedents = self.precedent_manager.search_relevant_precedents(clause_text, k=1)
        precedent_text = precedents[0] if precedents else "ê´€ë ¨ íŒë¡€ ì—†ìŒ"

        return f"=== [ê´€ë ¨ ë²•ë ¹] ===\n{law_text}\n\n=== [ê´€ë ¨ íŒë¡€] ===\n{precedent_text}"

    def detect(self, clause_text):
        """
        ì¡°í•­ì„ ë¶„ì„í•˜ì—¬ ë…ì†Œì¡°í•­ ì—¬ë¶€, ìœ„í—˜ ì ìˆ˜, ê·¼ê±°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        print(f"ğŸ•µï¸ ì¡°í•­ ë¶„ì„ ì¤‘: {clause_text[:30]}...")
        
        # 1. DB ê²€ìƒ‰ (Retrieval)
        retrieved_context = self._retrieve_context(clause_text)
        
        # 2. G-Eval í‰ê°€ (Metric ìƒì„±)
        toxic_metric = GEval(
            name="Contract Safety Score",
            criteria=self.toxic_criteria,
            rubric=self.rubric,
            evaluation_steps=self.evaluation_steps,
            model=self.evaluator_llm, # ì–´ëŒ‘í„° ì‚¬ìš©
            threshold=0.6, 
            evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.RETRIEVAL_CONTEXT]
        )

        # 3. Test Case ìƒì„±
        test_case = LLMTestCase(
            input=clause_text,
            actual_output="í‰ê°€ ëŒ€ìƒì…ë‹ˆë‹¤.", # G-Eval Output ë¶ˆí•„ìš”
            retrieval_context=[retrieved_context]
        )

        # 4. ì¸¡ì • ì‹¤í–‰
        toxic_metric.measure(test_case)
        
        # 5. ê²°ê³¼ í•´ì„ (Safety Score -> Risk Score ë³€í™˜)
        # G-Eval ì ìˆ˜(0~1)ëŠ” 'ì•ˆì „ë„'ë¥¼ ì˜ë¯¸í•˜ë¯€ë¡œ, 'ìœ„í—˜ë„'ëŠ” (1 - ì ìˆ˜)ë¡œ ê³„ì‚°
        safety_score = toxic_metric.score
        risk_score = 1.0 - safety_score
        
        # ìœ„í—˜ë„ê°€ 0.4(40%)ë¥¼ ì´ˆê³¼í•˜ë©´ ë…ì†Œì¡°í•­ìœ¼ë¡œ íŒë‹¨
        is_toxic = risk_score > 0.4 
        
        return {
            "clause": clause_text,
            "is_toxic": is_toxic,
            "risk_score": round(risk_score * 10, 1), # 10ì  ë§Œì  í™˜ì‚°
            "reason": toxic_metric.reason,
            "context_used": retrieved_context
        }

    def generate_easy_suggestion(self, detection_result):
        """
        íŒë³„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'ì‰¬ìš´ í•´ì„'ê³¼ 'ìˆ˜ì • ì œì•ˆ'ì„ ìƒì„±í•©ë‹ˆë‹¤. (Generator)
        """
        if not detection_result['is_toxic']:
            return "âœ… ë²•ì ìœ¼ë¡œ ë¬¸ì œì—†ëŠ” ì•ˆì „í•œ ì¡°í•­ì…ë‹ˆë‹¤."

        prompt = f"""
        ë‹¹ì‹ ì€ ê·¼ë¡œì í¸ì¸ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì•„ë˜ ì¡°í•­ì´ 'ë…ì†Œì¡°í•­'ìœ¼ë¡œ íŒë³„ë˜ì—ˆìŠµë‹ˆë‹¤.
        
        [ì›ë¬¸ ì¡°í•­]: {detection_result['clause']}
        [ìœ„í—˜ íŒë‹¨ ê·¼ê±°]: {detection_result['reason']}
        [ì°¸ê³  ë²•ë ¹/íŒë¡€]: {detection_result['context_used']}

        ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
        1. **ì‰¬ìš´ í•´ì„**: ì´ ì¡°í•­ì´ ì™œ ìœ„í—˜í•œì§€ ì´ˆë“±í•™ìƒë„ ì•Œê¸° ì‰½ê²Œ ì„¤ëª… (2ë¬¸ì¥ ì´ë‚´)
        2. **ìˆ˜ì • ì œì•ˆ**: ê·¼ë¡œìì—ê²Œ ìœ ë¦¬í•˜ê±°ë‚˜ ë²•ì— ë§ê²Œ ìˆ˜ì •í•œ ì¡°í•­ ì˜ˆì‹œ
        """
        
        return self.evaluator_llm.generate(prompt)